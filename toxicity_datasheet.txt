"Users’ Behavioral and Emotional Response to Toxicity in Twitter Conversations" Datasheet

Questions:
1. For what purpose was the dataset created?
The dataset has been created for the research project "Users’ Behavioral and Emotional Response to Toxicity in Twitter Conversations"  where we aimed to analyze users' emotional and behavioral responses to toxic replies. 

2. Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?
The dataset was created by the research paper authors: Ana Aleksandric, Sayak Saha Roy, Hanani Pankaj, Gabriela Mustata Wilson, Shirin Nilizadeh, on behalf of the University of Texas at Arlington.

3. Who funded the creation of the dataset?
No funding was used for this project.

4. Any other comments? 
None.

5. What do the instances that comprise the dataset represent (e.g.,
documents, photos, people, countries)? 
The dataset consists of unique tweet IDs (both original tweets and replies) that can be supplemented to Twitter API to obtain the full dataset.

6. How many instances are there in total (of each type, if appropriate)?
There are 528,041 tweets, posted by 328,390 unique users.

7. Does the dataset contain all possible instances or is it a sample
(not necessarily random) of instances from a larger set?
The dataset contains all instances.

8. What data does each instance consist of?
Each instance contains a conversation ID (ID of the main tweet), tweet ID, and toxicity score.

9. Is there a label or target associated with each instance?
No.

10. Is any information missing from individual instances? 
No.

11.  Are relationships between individual instances made explicit
(e.g., users’ movie ratings, social network links)?
Tweet IDs with the same conversation ID (AuthorTweet) belong to the same conversation.

12. Are there recommended data splits (e.g., training, development/validation,
testing)?
No, there are not.

13.  Are there any errors, sources of noise, or redundancies in the
dataset?
No, there are not.

14. Is the dataset self-contained, or does it link to or otherwise rely on
external resources (e.g., websites, tweets, other datasets)? 
Yes, each ID is a unique ID that might not necessarily exist in the future (tweets get deleted, users deactivate accounts, etc.). Therefore, such posts might not be obtainable. 

15. Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor–
patient confidentiality, data that includes the content of individuals’ non-public communications)?
No.

16. Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety?
If the whole dataset is obtained, there might be some posts that cause anxiety if viewed directly (as some tweet texts might be found very offensive by some).

17. Does the dataset identify any subpopulations (e.g., by age, gender)? I
No.

18. Is it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other
data) from the dataset? 
The tweet IDs in our dataset do not directly disclose the identity of the individual, but if they are hydrated using the Twitter API, then, it is possible to locate users' accounts and identify them.

19. Does the dataset contain data that might be considered sensitive
in any way (e.g., data that reveals race or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic
data; forms of government identification, such as social security
numbers; criminal history)? I
Some of the examples provided above might be visible from users' profiles. However, such information is not present in the metadata itself.

20. How was the data associated with each instance acquired?
The data is observable (it is coming from Twitter), therefore, it was extracted by using Twitter API.

21. What mechanisms or procedures were used to collect the data
(e.g., hardware apparatuses or sensors, manual human curation,
software programs, software APIs)? 
Twitter API and twarc were used to collect data.

22. If the dataset is a sample from a larger set, what was the sampling
strategy (e.g., deterministic, probabilistic with specific sampling
probabilities)?
Even though this dataset is not a part of a larger dataset, Twitter API provides only a 1% random sample of tweets.

23. Who was involved in the data collection process (e.g., students,
crowdworkers, contractors) and how were they compensated (e.g.,
how much were crowdworkers paid)?
Data was collected by the paper's authors. As stated above, no funding was provided for this project.

24. Over what timeframe was the data collected? 
August 14th to September 28th, 2021.

25. Were any ethical review processes conducted (e.g., by an institutional review board)?
No.

26. Did you collect the data from the individuals in question directly,
or obtain it via third parties or other sources (e.g., websites)?
We used Twitter API to obtain individual posts.

27. Did the individuals in question consent to the collection and use
of their data?
No, as the API is restricted to only sharing publicly available posts. Public posts are visible to anyone (see https://help.twitter.com/en/safety-and-security/public-and-protected-posts)

28. Has an analysis of the potential impact of the dataset and its use
on data subjects (e.g., a data protection impact analysis) been conducted?
No.

29. Was any preprocessing/cleaning/labeling of the data done (e.g.,
discretization or bucketing, tokenization, part-of-speech tagging,
SIFT feature extraction, removal of instances, processing of missing values)?
The text of each tweet was cleaned before passing the tweet to Google's Perspective API by replacing emojis and removing punctuation.

30. Was the “raw” data saved in addition to the preprocessed/cleaned/labeled
data (e.g., to support unanticipated future uses)?
No.

31. Is the software that was used to preprocess/clean/label the data
available?
No.

32. Has the dataset been used for any tasks already?
We performed propensity score matching and statistical analysis described in the research paper.

33. Is there a repository that links to any or all papers or systems that
use the dataset?
Here is the pre-print of the paper: https://arxiv.org/pdf/2210.13420.pdf

34. What (other) tasks could the dataset be used for?
The data can be used for multiple tasks, like analyzing the conversation dynamics more deeply in terms of users' reactions to toxic replies, as well as improving current state-of-the-art toxicity detection models by using whole conversations.

35. Is there anything about the composition of the dataset or the way
it was collected and preprocessed/cleaned/labeled that might impact future uses?
No.

36. Are there tasks for which the dataset should not be used?
No.

37. Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which
the dataset was created?
No.

38. How will the dataset will be distributed (e.g., tarball on website,
API, GitHub)?
The dataset will be shared on Zenodo and GitHub.

39. When will the dataset be distributed?
The dataset is already published.

40. Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use
(ToU)?
The dataset is made public (Creative Commons Attribution 4.0 International ).

41. Have any third parties imposed IP-based or other restrictions on
the data associated with the instances?
No.

42. Do any export controls or other regulatory restrictions apply to
the dataset or to individual instances? I
No.

43. Who will be supporting/hosting/maintaining the dataset?
The paper authors.

44. How can the owner/curator/manager of the dataset be contacted?
Via email, which can be found in the research paper.

45.Is there an erratum?
No.

46.Will the dataset be updated (e.g., to correct labeling errors, add
new instances, delete instances)?
No.

47. If the dataset relates to people, are there applicable limits on the
retention of the data associated with the instances (e.g., were the
individuals in question told that their data would be retained for
a fixed period of time and then deleted)?
No.

48. Will older versions of the dataset continue to be supported/hosted/maintained?
Currently, there are no other versions of the dataset.

49.If others want to extend/augment/build on/contribute to the
dataset, is there a mechanism for them to do so?
Other researchers can use the dataset for their research purposes.